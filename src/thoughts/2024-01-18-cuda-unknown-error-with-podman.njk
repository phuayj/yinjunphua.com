---
layout: layouts/thought.njk
title: "CUDA Unknown Error with Podman | Thoughts | Yin Jun, Phua -- Assistant Professor at Tokyo Tech"
heading: "CUDA Unknown Error with Podman"
lang: en
description: "Yin Jun Phua, currently an assistant professor at Tokyo Institute of Technology. Main research focus is on bridging the gap between symbolic AI and neural networks."
permalink: "/thoughts/2024-01-18-cuda-unknown-error-with-podman.html"
date: 2024-01-18
tags:
  - thoughts
---
{% block post %}
<p>As I was setting up my new deep learning workstation with Debian 12, I found out that <a href="https://docs.nvidia.com/ai-enterprise/deployment-guide-rhel-with-kvm/0.1.0/podman.html" target="_blank" rel="nofollow">Nvidia now supports Podman</a>. With all kinds of bad rumors surrounding Docker, I decided to try out Podman.</p>
<p>Once I had successfully navigated through the <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" target="_blank" rel="nofollow">installation process</a> and <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/cdi-support.html" target="_blank" rel="nofollow">set up CDI for Podman</a>, I was able to get a CUDA container up and running. Running <code>nvidia-smi</code> in the container confirmed that the GPU is available. However, when I tried to run any CUDA program, it resulted in an "Unknown error" message. The message itself contained no useful information, and I had no lead to even start troubleshooting this.</p>
<p>At first, I thought that maybe there were some limitations with the CUDA version on the host and the container. But despite running the same CUDA version, it still didn't work. Even turning to Google didn't yield anything helpful.</p>
<p>I had another Debian machine running CUDA in a Docker container just fine, so I figured I might as well check if there were any differences in the container. I stumbled upon a <a href="https://github.com/containers/podman/issues/9926#issuecomment-814665732" target="_blank" rel="nofollow">Github issue</a> that was for a totally different problem, but it suggested that not having the correct device files under <code>/dev</code> can cause CUDA to fail. This prompted me to look further into it.</p>
<p>Interestingly enough, there was a difference. The <code>/dev</code> folder in the Docker container included <code>/dev/nvidia-uvm</code> and <code>/dev/nvidia-uvm-tools</code>, whereas the Podman container did not. I realized that the <code>nvidia-uvm</code> kernel module wasn't loaded on the new workstation. So I tried to <code>modprobe nvidia-uvm</code> and check to see if it would solve the issue... and it didn't.</p>
<p>Turns out, manually loading the <code>nvidia-uvm</code> kernel module doesn't create the device files. So I found <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#device-node-verification" target="_blank" rel="nofollow">this script</a> on Nvidia's website that will create the necessary device files, and ran the relevant commands.</p>
<pre># grep nvidia-uvm /proc/devices
&lt;device-id&gt; nvidia-uvm
# mknod -m 666 /dev/nvidia-uvm c &lt;device-id&gt; 0
# mknod -m 666 /dev/nvidia-uvm-tools c &lt;device-id&gt; 1</pre>
<p>Then, after regenerating the CDI configuration file, it finally worked.</p>
<p><code>nvidia-uvm</code> seems to be loaded automatically when a program initializes CUDA. But for whatever reason, this wasn't happening with programs running within the Podman container. I have yet to determine whether this is down to the rootless container or needing the <code>--privileged</code> flag, but for now, manually loading the kernel module and creating the device files seems to fix the issue.</p>
{% endblock %}
